{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imports import * \n",
    "from functions import *\n",
    "from archs import *\n",
    "from mylearner import *\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '/scratch/smartairsense/data/'\n",
    "\n",
    "csv_file = os.path.join(data_folder,'df_minimal_clean.csv')\n",
    "df = pd.read_csv(csv_file,usecols=['humidity_abs','temperature','tvoc','oxygen','co2','co','no2','o3'],dtype=np.float32)\n",
    "df_test = df.iloc[df.shape[0]-500:] ## to get better visualization on less focused data\n",
    "df = df.drop(index = df.index[df.shape[0]-500:]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Missing values, drop NaN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_missing(df)\n",
    "\n",
    "#### substitute NaN values with mean \n",
    "## polynomial interpolation with degree > 1 uses index, also convert dtype to float to work\n",
    "impute_NaN(df)\n",
    "\n",
    "############## conmibe into df \n",
    "\n",
    "df.reset_index(drop=True,inplace= True)\n",
    "df.columns=df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sliding data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    hyperparams = yaml.load(f,SafeLoader)\n",
    "\n",
    "under_window = hyperparams['sample_segment']['under_window']\n",
    "seq_len = hyperparams['sample_segment']['seq_len']\n",
    "stride = hyperparams['sample_segment']['stride']\n",
    "sliding_mode = hyperparams['sample_segment']['sliding_mode']\n",
    "\n",
    "X= sliding(seq_len,stride,df,mode=sliding_mode)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**splitting and standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###### splitting\n",
    "# splits = TrainValidTestSplitter(valid_size=0.1,test_size=0.1)(y) ##### we have test set here\n",
    "splits = TrainValidTestSplitter(valid_size=0.1)(X[:,0,0]) ##### we DON'T have test set here\n",
    "x_train = np.zeros(X[splits[0]].shape,dtype=np.float32)\n",
    "x_valid = np.zeros(X[splits[1]].shape,dtype=np.float32)\n",
    "# x_test = np.zeros(X[splits[2]].shape,dtype=np.float32)\n",
    "# y_test = y[splits[2]]\n",
    "\n",
    "#different test set\n",
    "### uses stride = 1 to check all time points\n",
    "x_test_ = sliding(seq_len,1,df_test,mode=sliding_mode)\n",
    "x_test = np.zeros(x_test_.shape,dtype=np.float32) \n",
    "#################\n",
    "scalers = {}\n",
    "for i in range(x_train.shape[1]): ## n_features\n",
    "    scalers[i] = StandardScaler()\n",
    "    scalers[i].fit(np.unique((X[splits[0]])[:, i, :]).reshape(-1,1)) ### as we have overlapping samples\n",
    "    x_train[:, i, :] = scalers[i].transform((X[splits[0]])[:, i, :].reshape(-1,1)).reshape(x_train.shape[0],x_train.shape[-1])\n",
    "    x_valid[:, i, :] = scalers[i].transform((X[splits[1]])[:, i, :].reshape(-1,1)).reshape(x_valid.shape[0],x_valid.shape[-1])\n",
    "    # x_test[:, i, :] = scalers[i].transform((X[splits[2]])[:, i, :].reshape(-1,1)).reshape(x_test.shape[0],x_test.shape[-1]) ## from splitting\n",
    "    x_test[:, i, :] = scalers[i].transform((x_test_)[:, i, :].reshape(-1,1)).reshape(x_test.shape[0],x_test.shape[-1]) ## test set from outside\n",
    "print(x_train.shape,x_valid.shape,x_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plotting distribution of data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_lbs = ['train set','valid set','test set']\n",
    "y_nmrs = [x_train.shape[0],x_valid.shape[0],x_test.shape[0]]\n",
    "\n",
    "fig, ax = plt.subplots()    \n",
    "ind = np.arange(len(y_nmrs))  # the x locations for the groups\n",
    "bars = ax.bar(ind, y_nmrs, color=\"blue\")\n",
    "ax.set_xticks(ind)\n",
    "ax.set_xticklabels(x_lbs, minor=False)\n",
    "plt.title('Distribution of datset')\n",
    "# plt.xlabel('x')\n",
    "# plt.ylabel('y')\n",
    "ax.bar_label(bars)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**train cnn+lfstm autoencoder**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('config.yaml') as f:\n",
    "    hyperparams = yaml.load(f,SafeLoader)\n",
    "\n",
    "epochs = hyperparams['model']['epochs']\n",
    "bs = hyperparams['model']['bs']\n",
    "num_workers = hyperparams['model']['num_workers']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###training/validation dataloaders\n",
    "Tsets = TSDatasets(x_train, inplace=True)\n",
    "Vsets = TSDatasets(x_valid, inplace=True)\n",
    "dls   = TSDataLoaders.from_dsets(Tsets, Vsets, bs = bs, num_workers=num_workers)#,batch_tfms=batch_tfms) ### note the normalization\n",
    "\n",
    "## testing dataloaders\n",
    "Test_set = TSDatasets(x_test, inplace=True)\n",
    "Test_dls   = TSDataLoader(Test_set, bs = bs, num_workers=num_workers)#,batch_tfms=batch_tfms) ### note the normalization\n",
    "\n",
    "autoencoder = AutoEncoder(dls.vars)\n",
    "\n",
    "autoencoder, history = train_autoencoder(\n",
    "  autoencoder,\n",
    "  dls.train,\n",
    "  dls.valid,\n",
    "  n_epochs=100\n",
    ")\n",
    "\n",
    "### save autoencoder\n",
    "torch.save(autoencoder.state_dict(), f'models/{autoencoder._get_name()}.pt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**plot autoencoder train/valid losses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history['train'],label='train_loss')\n",
    "plt.plot(history['val'],label = 'valid_loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**predict for autoencoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions, pred_losses = predict_autoencoder(autoencoder, Test_dls)\n",
    "plt.figure()\n",
    "sns.distplot(pred_losses, bins=50, kde=True)\n",
    "plt.title('Distribution of reconstruction error of predictions')\n",
    "\n",
    "### correct predictions based on threshold for reconstruction error\n",
    "threshold = 0.2\n",
    "correct = sum(l <= threshold for l in pred_losses)\n",
    "print(f'Correct normal predictions: {correct}/{x_test.shape[0]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**visualizing features in a lower dimensional space [note output of encoder]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_dict = torch.load(f'models/{AutoEncoder.__name__}.pt')\n",
    "new_autoencoder = AutoEncoder(x_train.shape[1])\n",
    "new_autoencoder.load_state_dict(pretrained_dict)\n",
    "### get ptrtrained encoder\n",
    "enc = new_autoencoder.encoder\n",
    "#######################################################\n",
    "\n",
    "new_model = enc ##### for encoder_classifier\n",
    "\n",
    "###########################################\n",
    "###### get the features/predicions of the model_body\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "new_model.to(device)\n",
    "new_model.eval()\n",
    "feats = []\n",
    "lbls = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in Test_dls:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = new_model(inputs)\n",
    "        #####################################################\n",
    "        feats.append(outputs[:,-1]) #### for output of encoder\n",
    "\n",
    "        #######################################################\n",
    "        lbls.append(labels)\n",
    "\n",
    "test_feats = torch.cat(feats).detach().cpu().numpy()\n",
    "test_lbls = torch.cat(lbls).detach().cpu().numpy()\n",
    "\n",
    "#### get labels as strings\n",
    "person_labels = []\n",
    "person_dict = {1 : 'people', 0 : 'no person'}\n",
    "window_labels = []\n",
    "window_dict = {1 : 'open', 0 : 'closed'}\n",
    "for item in test_lbls:\n",
    "    person_labels.append(person_dict[item[0]])\n",
    "    window_labels.append(window_dict[item[1]])\n",
    "\n",
    "##########\n",
    "## using tensorboard projector\n",
    "# writer = SummaryWriter('runs/')\n",
    "# writer.add_embedding(test_feats,metadata=person_labels,tag = f'person_embeddings_{learn.model._get_name()}')\n",
    "# writer.add_embedding(test_feats,metadata=window_labels,tag = f'window_embeddings_{learn.model._get_name()}')\n",
    "# writer.close()\n",
    "##########\n",
    "### using sklearn and plotly\n",
    "components = visualize_embeddings(test_feats,person_labels,window_labels,n_components=2,method='pca')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9896e4719c177e73e0a92dd62787cd74e9c19ae40be169094c16f2e556b6c4a5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
